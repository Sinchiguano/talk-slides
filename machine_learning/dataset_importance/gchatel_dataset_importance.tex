\documentclass[10pt]{beamer}

\input{packages.tex}
\input{colors.tex}

\usetheme{Boadilla}
\title{Importance of dataset for learning algorithms}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Disaitek\\Intel AI Software Innovator\\\vspace{0.3cm}@rodgzilla\\github.com/rodgzilla}
\date{September 25th, 2018}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \tableofcontents

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Machine learning}

  \framesubtitle{Supervised learning}

  Machine learning is a subfield of artificial intelligence.

  \bigskip

  \begin{description}
    \item[Intuitively] We want to \emph{learn from} and \emph{make predictions
    on} data.

    \medskip

    \item[Technically] We want to build a model that approximate well
      (\textit{e.g.} minimize a loss function) an unknown function for
      which we only have limited observations.
  \end{description}

  \bigskip

  To do this, we usually need a lot of \emph{data}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%o


\section{Popular ML tasks and their dataset}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Popular datasets for computer vision}

  \begin{description}[labelwidth=\widthof{bf series 2017, JFT-300M}]
    \setlength{\itemsep}{8pt}
    \item[1990, Statlog] $\sim$2k outdoor images
    \item[1998, MNIST] 60k B&W images of handwritten digits
    \item[2005, LabelMe] $\sim$187k scenes images
    \item[2009, ImageNet] $\sim$14M color images
    \item[2017, JFT-300M] $\sim$300M color images (internal dataset @ Google)
  \end{description}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Popular datasets for Natural language processing}

  \begin{description}[labelwidth=\widthof{bf series 2017, JFT-300M}]
    \setlength{\itemsep}{8pt}
    \item[1997, Car evaluation dataset] $\sim$2k car evaluations
    \item[2005, Stanford Sentiment Treebank] $\sim$11k movie reviews
    \item[2011, IMDB Reviews] $\sim$50k movie reviews
    \item[2012, Youtube Comedy Slam] $\sim$1.1M pairs of video metadata
    \item[2015, Amazon reviews] $\sim$82m product reviews
  \end{description}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Creating dataset}

  Creating new high quality datasets is both hard and expensive.

  \bigskip

  Some researchers experiment with training models using low quality
  data (weakly supervised learning)

  \bigskip

  \textbf{TODO: AMT}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data efficiency}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Data efficiency}

  Knowing that datasets are so important and hard to create, it is
  important to squeeze every last bit of value out of them.

  \bigskip

  To do this, three ideas are explored:

  \begin{itemize}
    \item Transfer learning
    \item Multi-task learning
    \item Semi-supervised learning
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Transfer learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transfer learning}

  \begin{displayquote}[Perkins, 1992]
  The application of skills, knowledge, and/or attitudes that were
  learned in one situation to another learning situation.
  \end{displayquote}

  \bigskip

  Transfer learning consists in taking an artificial neural network
  that has been trained on a \textcolor{blue}{\emph{generic}} task and
  \textcolor{blue}{\emph{transferring}} its knowledge (retraining it)
  to perform a new task.

  \bigskip

  The idea behind this method is that the information learned on a
  generic task will probably be useful for a new task of the same
  domain.

  \bigskip

  Transfer learning is actually the base of the
  \textcolor{blue}{Google Cloud AutoML service}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transfer learning in computer vision}

  Finetuning an ImageNet model (1000 categories classification) to
  easily perform cats vs dogs classification.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transfer learning in NLP}

  Language modeling as a generic task

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multi-task learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning}

  \begin{displayquote}[Rich Caruana, 1997]
    Multitask Learning is an approach to inductive transfer that
    improves generalization by using the domain information contained
    in the training signals of related tasks as an inductive bias. It
    does this by learning tasks in parallel while using a shared
    representation; what is learned for each task can help other tasks
    be learned better.
  \end{displayquote}

  \bigskip

  Instead of just training the network to perform the desired task, we
  also optimize it to perform \textcolor{blue}{\emph{auxiliary
      tasks}}.

  \begin{center}
    \includegraphics[width = 4.5cm]{images/multi_task_learning.png}
  \end{center}

  {\scriptsize \textit{Image from http://ruder.io/multi-task/}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task as a regularization technique}

  Informally, the goal of the multi-task learning is to force the
  model to use its \textcolor{blue}{computing power} to perform
  something \textcolor{blue}{meaningful} instead of using it to learn
  the \textcolor{blue}{noise} of the data (overfitting).
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning in computer vision}

  Some researchers noticed that by asking the model to predict the
  gender and age of patient in addition to detect \emph{diabetic
    retinopathy} they got strong performance improvements.

  \textbf{Find ref for the research paper that predicts gender from
    eye picture as auxiliary task for diabetic retinopathy detection.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning in NLP}

  Language modeling (ref OpenAI paper)

  \bigskip

  Translation for PoS tagging (syntactic tree?) (ref ``one model to
  learn them all'' for an extreme example).
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Semi-supervised learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Semi-supervised learning}

  \begin{center}
    \includegraphics[width=4cm]{images/semi_supervised_learning.png}
  \end{center}

  \bigskip

  The idea of semi-supervised learning is to use
  \textcolor{blue}{\emph{unlabelled data}} to improve our model.

  \bigskip

  {\scriptsize \textit{Image from https://en.wikipedia.org/wiki/Semi-supervised\_learning}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Ideas of semi-supervised learning}

  The main concept of semi-supervised learning is to train a
  \textcolor{blue}{weaker student} to imitate a
  \textcolor{blue}{stronger teacher}.

  \bigskip

  Technically, we apply \emph{mean-squared error} or a
  \emph{Kullback-Liebler divergence} between the logits output by the
  student and the teacher. We typically alternate between supervised
  and semi-supervised steps of training.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Semi-supervised learning in practice}

  To do this, we can for example take the model we are training and
  train it to make the same prediction on a clean and noisy version of
  a same image. In this case, the prediction on the \emph{noisy image}
  (resp.\ \emph{clean image}) is the one of the
  \textcolor{blue}{student model} (resp.\ teacher model)

  \bigskip

  To apply this algorithm, we do not need the real label of the image,
  we just assume that the model is already giving the right one and
  improve it by forcing it to acquire \textcolor{red}{noise
    invariance}.

  \bigskip

  \textbf{TODO: Find the correct ref and a picture from the curious
    company blog post about mean teachers}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
