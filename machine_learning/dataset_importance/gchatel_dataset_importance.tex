\documentclass[10pt]{beamer}

\input{packages.tex}
\input{colors.tex}

\usetheme{Boadilla}
\title{Importance of dataset for learning algorithms}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Disaitek\\Intel AI Software Innovator\\\vspace{0.3cm}@rodgzilla\\github.com/rodgzilla}
\date{September 25th, 2018}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \tableofcontents

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Machine learning}

  \framesubtitle{Supervised learning}

  Machine learning is a subfield of artificial intelligence.

  \bigskip

  \begin{description}
    \item[Intuitively] We want to \emph{learn from} and \emph{make predictions
    on} data.

    \medskip

    \item[Technically] We want to build a model that approximate well
      (\textit{e.g.} minimize a loss function) an unknown function for
      which we only have limited observations.
  \end{description}

  \bigskip

  To do this, we usually need a lot of \emph{data}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%o


\section{Popular ML tasks and their dataset}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Popular datasets for computer vision}

  \begin{description}[labelwidth=\widthof{bf series 2017, JFT-300M}]
    \setlength{\itemsep}{8pt}
    \item[1990, Statlog] $\sim$2k outdoor images
    \item[1998, MNIST] 60k B&W images of handwritten digits
    \item[2005, LabelMe] $\sim$187k scenes images
    \item[2009, ImageNet] $\sim$14M color images
    \item[2017, JFT-300M] $\sim$300M color images (internal dataset @ Google)
  \end{description}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Popular datasets for Natural language processing}

  \begin{description}[labelwidth=\widthof{bf series 2017, JFT-300M}]
    \setlength{\itemsep}{8pt}
    \item[1997, Car evaluation dataset] $\sim$2k car evaluations
    \item[2005, Stanford Sentiment Treebank] $\sim$11k movie reviews
    \item[2011, IMDB Reviews] $\sim$50k movie reviews
    \item[2012, Youtube Comedy Slam] $\sim$1.1M pairs of video metadata
    \item[2015, Amazon reviews] $\sim$82m product reviews
  \end{description}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \section{Specific problem of dataset for NLP tasks}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}

%%   \frametitle{NLP tasks specificity}

%% \end{frame}
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}

%%   \frametitle{NLP tasks bias}

%%   There are problems with using services such AMT (Amazon Mechanical
%%   Turk) to annotate NLP dataset.

%% \end{frame}
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data efficiency}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Data efficiency}

  Knowing that data is so important and hard to produce, it is
  important to squeeze every last bit of value out of it.

  \bigskip

  To do this, three ideas are explored:
  \begin{itemize}
    \item Transfer learning
    \item Multi-task learning
    \item Semi supervision
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Transfer learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transfer learning}

  \begin{displayquote}[Perkins, 1992]
  The application of skills, knowledge, and/or attitudes that were
  learned in one situation to another learning situation.
  \end{displayquote}

  \bigskip

  Transfer learning consists in taking an artificial neural network
  that has been trained on a \textcolor{blue}{\emph{generic}} task and
  \textcolor{blue}{\emph{transferring}} its knowledge (retraining it)
  to perform a new task.

  \bigskip

  The idea behind the method is that the information learned on a
  generic task will probably be useful for a new task of the same
  domain.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Pre-training in computer vision}

  Finetuning an ImageNet model (1000 categories classification) to
  easily perform cats vs dogs classification.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Pre-training in NLP}

  Language modeling as a generic task

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multi-task learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning}

  \begin{displayquote}[Rich Caruana, 1997]
    Multitask Learning is an approach to inductive transfer that
    improves generalization by using the domain information contained
    in the training signals of related tasks as an inductive bias. It
    does this by learning tasks in parallel while using a shared
    representation; what is learned for each task can help other tasks
    be learned better.
  \end{displayquote}

  \bigskip

  Instead of just training the network to perform the desired task, we
  also optimize it to perform \textcolor{blue}{\emph{auxiliary
      tasks}}.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning in computer vision}

  \textbf{Find ref for the research paper that predicts gender from
    eye picture as auxiliary task for diabetic retinopathy detection.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Semi supervision}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Semi-supervision}

  \begin{center}
    \includegraphics[width=4cm]{images/semi_supervised_learning.png}
  \end{center}

  \bigskip

  The idea of semi-supervised learning is to use
  \textcolor{blue}{\emph{unlabelled data}} to improve our model.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Rappel des fondamentaux (optimisation du fonction de perte qui nécessite de confronter des données labelées avec les prédictions des modèles).
%% Présentation de la taille et de la qualité des jeux de données pour les disciplines classiques du machine learning.
%% Problématique en NLP pour la qualité et la quantité de données labelées
%% Solution : Utilisation des données non labelées pour améliorer les modèles
%% Pré-entrainement des modèles sur des tâches non-supervisées
%%  Semi-supervision en utilisant les données non labelées

\end{document}
