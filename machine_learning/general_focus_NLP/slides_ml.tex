\documentclass[9pt]{beamer}

\input{packages.tex}
\input{colors.tex}

\usetheme{Boadilla}
\title{Machine learning basics}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Disaitek\\Intel Software Innovator\\\vspace{0.3cm}@rodgzilla\\github.com/rodgzilla\\\vspace{.5cm}\includegraphics[width = 2.5cm]{images/logo_disaitek.png} \vspace{-1cm}
}
\date{2019/02/12}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Machine learning}

  Machine learning (ML) is a subfield of artificial intelligence.

  \bigskip

  \begin{description}
    \item[Intuitively] We want to \emph{learn from} and \emph{make predictions
    on} data.

      \medskip

    \item[Technically] We want to update the parameters of a model to
      make it describe our training data as well as possible (``well''
      being defined by a \emph{loss function}).
  \end{description}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model example}

  \framesubtitle{Linear regression}

  \begin{center}
    \scalebox{1.3}{
      \input{figures/scatter_02.tex}
    }
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model example}

  \framesubtitle{Decision tree}

  \begin{center}
    \includegraphics[width = 9cm]{images/decision_tree.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model example}

  \framesubtitle{Neural network (deep learning)}

  \begin{center}
    \scalebox{0.7}{
      \input{figures/network.tex}
    }
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Deep learning architecture}

  \framesubtitle{Image recognition (VGG 16)}

  \begin{center}
    \includegraphics[width = 9cm]{images/vgg16_architecture.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Deep learning architecture}

  \framesubtitle{Hierarchized pattern recognition}

  \begin{center}
    \begin{tikzpicture}[xscale = 5.5, yscale = 2.5]
      \onslide<1>{
        \node (L1) at (0, 0) {
          \includegraphics[width = 5cm]{images/cnn_vizu_l1.jpg}
        };

        \node (T1) at (-1, 0.5) {
          Layer 1
        };
      }
      \onslide<2>{
        \node (L2) at (0, 0) {
          \includegraphics[width = 5cm]{images/cnn_vizu_l2.jpg}
        };

        \node (T2) at (-1, 0.5) {
          Layer 2
        };
            }
      \onslide<3>{
        \node (L3) at (0, 0) {
          \includegraphics[width = 5cm]{images/cnn_vizu_l3.jpg}
        };

        \node (T3) at (-1, 0.5) {
          Layer 3
        };
            }
      \onslide<4>{
        \node (L4) at (0, 0) {
          \includegraphics[width = 5cm]{images/cnn_vizu_l4.jpg}
        };
        \node (T4) at (-1, 0.5) {
          Layer 4
        };
            }
      \onslide<5>{
        \node (L5) at (0, 0) {
          \includegraphics[width = 5cm]{images/cnn_vizu_l5.jpg}
        };

        \node (T5) at (-1, 0.5) {
          Layer 5
        };
      }

      \node (VGG) at (-1, -0.5) {
        \includegraphics[width = 6cm]{images/vgg16_architecture.png}
      };
    \end{tikzpicture}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Application examples}

  \framesubtitle{Supervised learning}

  \begin{itemize}
    \item Supervised tasks
      \begin{itemize}
      \item Regression

        \begin{center}
          \begin{tabular}{cc}
            \textcolor{blue}{Recommender system} & (user, book) $\to$ rating \\[0.2cm]
            \textcolor{blue}{House price} & (surface, nb rooms, city) $\to$ price \\[0.2cm]
        \end{tabular}
        \end{center}

      \item Classification

        \begin{center}
          \begin{tabular}{cc}
            \textcolor{blue}{Image classification} & pixel values $\to$ cat or dog \\[0.2cm]
            \textcolor{blue}{Text classification} & list of words $\to$ spam or valid email
          \end{tabular}
        \end{center}
      \end{itemize}
    \item Unsupervised tasks

      \begin{itemize}
        \item Clustering
          \begin{center}
            \textcolor{blue}{Group clients by interests} \\[.5cm]
          \end{center}
        \item Anomaly detection
          \begin{center}
            \textcolor{blue}{Credit card fraud detection}
          \end{center}
      \end{itemize}
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Deep Natural Language Processing (NLP)}

  \framesubtitle{Main ideas}

  \begin{itemize}
    \item Learning the \textcolor{blue}{semantic meaning} of words,
      \pause
      \bigskip
    \item Understanding the \textcolor{blue}{information hierarchy} related to
      the task at hand,
      \bigskip
      \pause
    \item Ability to make use of \textcolor{blue}{huge amounts of data}.
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Word embeddings}

  \framesubtitle{Semantic vectors}

  We associate to each word of the vocabulary a vector which
  represents its \textcolor{blue}{meaning}.

  \begin{center}
    \begin{description}
      \item[Oven] $[-0.2, 0.6]$
      \item[Microwave] $[-0.05, 0.57]$
      \item[Garden] $[0.22, -0.5]$
      \item \dots
    \end{description}
  \end{center}

  \vspace{-.9cm}

  \begin{center}
    \includegraphics[width = 8.5cm]{images/word_embeddings_5.png}
  \end{center}

  \vspace{-0.5cm}

  In real applications word embedding have 100 to 300 values,
  encoding all kind of characteristics about words.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Word embeddings}

  \framesubtitle{Links between concepts}

  When word embeddings are created using a large enough dataset, a lot
  of information is encoded in \textcolor{blue}{differences} between
  vectors.

  \begin{center}
    \includegraphics[width = 8cm]{images/word_embeddings_3.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Word embeddings}

  \framesubtitle{Vector geometry}

  \begin{center}
    \includegraphics[width = 5cm]{images/word_embeddings_2.png}
  \end{center}
  \[
  king - man + woman = queen
  \]
  \begin{center}
    \includegraphics[width = 3.5cm]{images/king_queen.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Word embeddings}

  \framesubtitle{Bias in representations}

  \begin{center}
    \includegraphics[width = 10cm]{images/nurse.png} \\[.5cm]
    \includegraphics[width = 10cm]{images/firefighter.png} \\[.5cm]
    \includegraphics[width = 10cm]{images/doctor.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{NLP tasks}

  \framesubtitle{Sentiment analysis}

  Automatized analysis of an item public perception:

  {\small
    \begin{itemize}
      \item Negative
        \begin{itemize}
          \item \textcolor{red}{Even fans of Ismail Merchant's work, I suspect,
            would have a hard time sitting through this one.}
          \item \textcolor{red}{Every conceivable mistake a director
            could make in filming opera has been perpetrated here.}
          \item \textcolor{red}{Cheap, vulgar dialogue and a plot that
            crawls along at a snail's pace.}
          \item \textcolor{red}{The material and the production itself
            are little more than routine.}
        \end{itemize}
      \item Positive
        \begin{itemize}
          \item \textcolor{cyan}{A rare and lightly entertaining look
            behind the curtain that separates comics from the people
            laughing in the crowd.}
          \item \textcolor{cyan}{Rarely, indeed almost never, is such
            high-wattage brainpower coupled with pitch-perfect acting
            and an exquisite, unfakable sense of cinema.}
          \item\textcolor{cyan}{Easily the most thoughtful fictional
            examination of the root causes of anti-Semitism ever seen
            on screen.}
        \end{itemize}
    \end{itemize}
  }
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{NLP tasks}

  \framesubtitle{Document tagging}

  Automatic tagging of documents, articles or books.

  \bigskip

  \begin{itemize}
  \item Supervised way using classification (using past labels):
    \begin{itemize}
    \item Harry Potter: \textcolor{cyan}{Child book}, \textcolor{blue}{Fantasy},
      \textcolor{red}{Aventure}, \dots
    \item Lord Of The Rings: \textcolor{blue}{Fantasy},
      \textcolor{red}{Aventure}, \dots
    \item Algorithms To Live By: \textcolor{darkgray}{Computer
      science}, \textcolor{lightgray}{Textbook}, \dots
    \end{itemize}

    \bigskip

  \item Unsupervised way using clustering (grouping books that looks the same):
    \begin{itemize}
    \item \textcolor{magenta}{Cluster 1}: Harry potter, Lord Of The
      Rings, \dots
    \item \textcolor{olive}{Cluster 2}: Algorithms To Live By, The Art
      of Computer Programming, \dots
    \item \textcolor{purple}{Cluster 3}: Tofu from Scratch, Okinawa
      Diet
    \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{NLP tasks}

  \framesubtitle{Search engine}

  By using the \textcolor{blue}{natural language understanding}
  capabilities of deep learning models, we can create more robust and
  performant search engines.

  \bigskip

  The matching performed by these search engines is
  \textcolor{blue}{semantic} (\textit{meaning} of the query) instead
  of \textcolor{red}{lexical} (finding \textit{exactly} the word of
  the query in documents).

  \bigskip

  \begin{center}
    \includegraphics[width = 8cm]{images/word_embeddings_4.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{NLP tasks}

  \framesubtitle{Automatic summarization}

  \begin{center}
    Extractive summarization (copy-paste most important sentences)\\[.3cm]
    \includegraphics[width = 5.5cm]{images/extractive_summarization.png} \\[.5cm]
    Abstractive summarization (generate new sentences that synthesize information)\\[.3cm]
    \includegraphics[width = 5.5cm]{images/abstractive_summarization.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{NLP tasks}

  \framesubtitle{Abstractive summarization using wikipedia}

  The goal is to generate the abstract (first few paragraphs) of
  Wikipedia articles from their source documents.

  \begin{center}
    \includegraphics[width = 10cm]{images/wikipedia_summarization.png}
  \end{center}

  \bigskip

  {\footnotesize \textit{Generating Wikipedia By Summarizing Long
      Sequences, Google Brain}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{NLP tasks}

  \framesubtitle{Unsupervised abstractive summarization}

  \begin{center}
    \includegraphics[width = 7cm]{images/unsupervised_summarization.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Language modeling}

  The goal of language modeling is to predict the word that is
  \textcolor{blue}{most likely} to appear after a given sequence of
  words.

  \begin{figure}
    \scalebox{0.7}{
      \input{figures/language_modeling.tex}
    }
  \end{figure}

  \begin{itemize}
  \item Allows to learn the syntax of a language.
  \item Allows to learn the semantic of the words.
  \item Unlimited amount of data.
  \item Can be trained in multilingual setting.
  \end{itemize}

  \medskip

  The idea is to \textcolor{blue}{pretrain} a model using this task
  and then use what it has learned to perform some other tasks.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Google BERT}

  The \textbf{B}idirectional \textbf{E}ncoder \textbf{R}epresentations
  for \textbf{T}ransformers model is a language model that has been
  trained on a massive corpus (7000 books + all Wikipedia pages for
  102 languages). It is used as a \textcolor{blue}{base} to perform
  many other linguistic tasks.

  \bigskip

  It allows user to work easily in a \textcolor{blue}{wide variety of
    languages}.

  \bigskip

  It produces new state of the art results on \textcolor{blue}{11 NLP
    tasks}.

  \bigskip

  It is very fast and inexpensive to \textcolor{blue}{transfer} on a
  new task.

  \bigskip

  A multilingual BERT model trained on a \textcolor{blue}{monolingual
    dataset} will work (although with lower performances than with a
  proper training) on \textcolor{blue}{another language} (zero-shot
  learning).
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{References}

  \fontsize{6pt}{7.2}\selectfont

  \begin{itemize}
  \item IMDB movie review classification: \href{https://github.com/rodgzilla/pytorch-openai-transformer-lm/tree/movie_reviews_classification}{\textcolor{blue}{Github repo for IMDB sentiment analysis with GPT}}

  \item Convolution layer visualization: \href{https://www.matthewzeiler.com/research.html}{\textcolor{blue}{matthewzeiler.com}}

  \item Mikolov, Tomas, et al. "Distributed representations of words
    and phrases and their compositionality." Advances in neural
    information processing systems. 2013.

  \item Google blog post about gender bias in Google Translate:
    \href{https://www.blog.google/products/translate/reducing-gender-bias-google-translate/}{\textcolor{blue}{Reducing gender bias in Google Translate}} (2018)

  \item Chu, Eric, and Peter J. Liu. "Unsupervised Neural
    Multi-document Abstractive Summarization." arXiv preprint
    arXiv:1810.05739 (2018).

  \item Liu, Peter J., et al. "Generating wikipedia by summarizing
    long sequences." arXiv preprint arXiv:1801.10198 (2018).

  \item Devlin, Jacob, et al. "BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding." arXiv
    preprint arXiv:1810.04805 (2018).

    \bigskip

  \item \href{https://github.com/rodgzilla/talk-slides/tree/master/machine_learning/general_focus_NLP}{\textcolor{blue}{Github repo for these slides.}}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Word2vec algorithm}

  The goal of the word2vec task is to predict the
  \textcolor{blue}{context} of word (the words surrounding it) based
  on its vector representation.

  \begin{center}
    \includegraphics[width = 4cm]{images/w2v.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Unsupervised summarization model}

  The abstractive summary is generated by decoding the
  \textcolor{blue}{mean representation} of the input documents.

  \begin{center}
    \includegraphics[width = 12cm]{images/unsupervised_summarization_model.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Google BERT for NLP tasks}

  To perform other tasks with BERT, you need to format your input and
  objective according to the way described in the article.

  \begin{center}
    \includegraphics[width = 7cm]{images/bert.png}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
