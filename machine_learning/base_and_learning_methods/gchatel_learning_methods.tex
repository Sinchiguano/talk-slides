\documentclass[10pt]{beamer}

\input{packages.tex}
\input{colors.tex}

\usetheme{Boadilla}
\title{Learning methods for limited datasets}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Disaitek\\Intel AI Software Innovator\\\vspace{0.3cm}gchatel@disaitek.com\\@rodgzilla\\github.com/rodgzilla}
\date{November 21st, 2018}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}

%% \usebackgroundtemplate{
%%   \includegraphics[width=\paperwidth,height=\paperheight]{images/logo_disaitek.png}
%% }

%% \usebackgroundtemplate{
%%   \rule{-4.5cm}{\paperheight}
%%   \hspace*{\paperwidth}
%%   \makebox[0pt][r]{
%%     %% \scalebox{0.7}{
%%       \includegraphics[width=\paperwidth,height=\paperheight]{images/logo_disaitek.png}
%%     %% }
%%   }
%% }

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \tableofcontents

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Machine learning}

  \framesubtitle{Supervised learning}

  Machine learning is a subfield of artificial intelligence.

  \bigskip

  \begin{center}
    \scalebox{0.6}{
      \input{figures/scatter.tex}
    }
  \end{center}

  \begin{description}
    \item[Intuitively] We want to \emph{learn from} and \emph{make predictions
    on} data.

    \medskip

    \item[Technically] We want to build a model that approximate well
      (\textit{e.g.} minimize a loss function) an unknown function for
      which we only have limited observations.
  \end{description}

  \bigskip

  To do this, we usually need \emph{a lot of data}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%o

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural network}

  \framesubtitle{Neuron with activation}

  \begin{center}
    \scalebox{0.6}{
      \input{figures/nonlinear_neuron.tex}
    }
  \end{center}

  \begin{center}
    \scalebox{0.8}{
      \begin{tikzpicture}[xscale = 4]
        \node (formula) (0, 0){
          \scalebox{1.3}{
            $A(x) = \frac{1}{1 + e^{-x}}$
          }
        } ;

        \node (graph) at (1, 0){
          \scalebox{0.6}{
            \input{figures/sigmoid.tex}
          }
        };
      \end{tikzpicture}
    }
  \end{center}

  \vspace{-0.5cm}

  \[
  y = A(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + w_{5}x_{5} + w_{6}x_{6} + b)
  \]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural network}

  \framesubtitle{Network}

  \begin{center}
    \scalebox{0.7}{
      \input{figures/network_animation.tex}
    }
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural network}

  \framesubtitle{Training}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/parabola.tex}
    }
  \end{center}

  \smallskip

  A differentiable loss function $L(\theta, x, y)$ tells us how well
  the model is performing. We compute how parameters changes influence
  the loss by taking its gradient $\nabla_{\theta} L(\theta, x, y)$.

  \medskip

  We then update the weights to make the value of the loss function
  decrease by iterating this formula:

  \[
  \theta_{t + 1} = \theta_{t} - \alpha \nabla_{\theta_{t}} L(\theta_{t}, x, y).
  \]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Popular ML tasks and their dataset}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Popular datasets}

  \framesubtitle{Computer vision}

  \begin{description}[labelwidth=\widthof{bf series 2017, JFT-300M}]
    \setlength{\itemsep}{8pt}
    \item[1990, Statlog] $\sim$2k outdoor images,
    \item[1998, MNIST] 60k B\&W images of handwritten digits,
    \item[2005, LabelMe] $\sim$187k scenes images,
    \item[2009, ImageNet] $\sim$14M images with 1000 different categories,
    \item[2017, JFT-300M] $\sim$300M RGB images $\sim$18k categories (internal dataset @ Google).
  \end{description}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Popular datasets}

  \framesubtitle{Natural language processing (NLP)}

  \begin{description}[labelwidth=\widthof{bf series 2017, JFT-300M}]
    \setlength{\itemsep}{8pt}
    \item[1997, Car evaluation dataset] $\sim$2k textual car evaluations,
    \item[2005, Stanford Sentiment Treebank] $\sim$11k movie reviews,
    \item[2011, IMDB Reviews] $\sim$50k movie reviews,
    \item[2012, Youtube Comedy Slam] $\sim$1.1M pairs of video metadata,
    \item[2015, Amazon reviews] $\sim$82M product reviews.
  \end{description}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Creating dataset}

  Creating new high quality datasets is both \textcolor{blue}{hard}
  and \textcolor{blue}{expensive}.

  \bigskip

  Some researchers experiment with training models using
  \textcolor{blue}{low quality data} (weakly supervised learning).

  \bigskip

  Amazon offers a \textcolor{blue}{dataset creation service} (Amazon
  Mechanical Turk) where you can pay to get your dataset labelled by
  humans.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data efficiency}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Data efficiency}

  Knowing that datasets are so important and hard to create, it is
  important to squeeze \emph{every last bit of value} out of them.

  \bigskip

  To do this, three ideas are explored:

  \begin{itemize}
    \item Transfer learning,
    \item Multi-task learning,
    \item Semi-supervised learning.
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Transfer learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transfer learning}

  \framesubtitle{Concept}

  \begin{displayquote}[Perkins, 1992]
  The application of skills, knowledge, and/or attitudes that were
  learned in one situation to another learning situation.
  \end{displayquote}

  \bigskip

  Transfer learning consists in taking an artificial neural network
  that has been trained on a \textcolor{blue}{\emph{generic}} task and
  \textcolor{blue}{\emph{transferring}} its knowledge (retraining it)
  to perform a new task.

  \bigskip

  The idea behind this method is that the information learned on a
  generic task will probably be useful for a new task of the same
  domain.

  \bigskip

  Transfer learning is actually the base of the
  \textcolor{blue}{Google Cloud AutoML service}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transfer learning}

  \framesubtitle{Computer vision}

  \begin{center}
    \includegraphics[width = 7cm]{images/alexnet_architecture.png}
  \end{center}

  Using this trained model as a base to build a dogs vs cats picture
  classifier \textcolor{blue}{greatly reduce the need of labelled
    data}.

  \bigskip

  The knowledge about \textcolor{blue}{basic shapes and textures} that
  has been learned on ImageNet will be useful to almost all task
  involving real world images.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transfer learning}

  \framesubtitle{Natural language processing}

  \begin{center}
    \scalebox{0.7}{
      \input{figures/language_modeling.tex}
    }
  \end{center}

  The \textcolor{blue}{language modeling task} is currently the most
  generic task that NLP researcher have found to perform transfer
  learning.

  \bigskip

  Knowing how to predict the most likely following word requires to
  understand, to some extent, the \textcolor{blue}{meaning of words},
  \textcolor{blue}{the syntax of the language} and
  \textcolor{blue}{the way concepts interact}.

  \bigskip

  Typical language models are trained on Wikipedia content, books or
  Internet Common Crawl.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multi-task learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning}

  \framesubtitle{Concept}

  \bigskip

  %% \begin{displayquote}[Rich Caruana, 1997]
  %%   Multitask Learning is an approach to inductive transfer that
  %%   improves generalization by using the domain information contained
  %%   in the training signals of related tasks as an inductive bias. It
  %%   does this by learning tasks in parallel while using a shared
  %%   representation; what is learned for each task can help other tasks
  %%   be learned better.
  %% \end{displayquote}

  \begin{displayquote}[Rich Caruana, 1997]
    Multi-task learning is an approach to inductive transfer that
    improves generalization [...] It
    does this by learning tasks in parallel while using a shared
    representation; what is learned for each task can help other tasks
    be learned better.
  \end{displayquote}

  \bigskip

  Instead of just training the network to perform the desired task, we
  also optimize it to perform \textcolor{blue}{\emph{auxiliary
      tasks}}.

  \begin{center}
    \includegraphics[width = 4.5cm]{images/multi_task_learning.png}
  \end{center}

  {\scriptsize \textit{Image from http://ruder.io/multi-task/}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning}

  \framesubtitle{Regularization technique}

  \begin{center}
    \includegraphics[width = 8cm]{images/overfitting.png}
  \end{center}

  Informally, the goal of the multi-task learning is to force the
  model to use its \textcolor{blue}{computing power} to perform
  something \textcolor{blue}{meaningful} instead of using it to learn
  the \textcolor{blue}{noise} of the data (overfitting).

  \bigskip

  {\scriptsize \textit{Image from
      https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning}

  \framesubtitle{Computer vision}

  \begin{center}
    \includegraphics[width = 5.cm]{images/multi_task_medecine.png}
  \end{center}

  \medskip

  Some researchers discovered that by asking a model to predict the
  gender and age of patient in addition to detect \emph{cardiovascular
    diseases} they got strong performance improvements.

  \medskip

  {\scriptsize Poplin, Ryan, et al. "Predicting cardiovascular risk
    factors from retinal fundus photographs using deep learning."
    arXiv preprint arXiv:1708.09843 (2017).}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multi-task learning}

  \framesubtitle{Natural language processing}

  \begin{center}
    \includegraphics[width = 4cm]{images/multi_task_nlp.png}
  \end{center}

  In NLP, translation can be used as an auxiliary task to improve
  models that perform tasks that have relatively small datasets such
  as sentence parsing.

  \bigskip

  By making the model perform translation, a task with huge datasets,
  we allow it to gain access to a much richer
  \textcolor{blue}{structure of the language}.

  \bigskip

  {\scriptsize Kaiser, Lukasz, et al. "One model to learn them all."
    arXiv preprint arXiv:1706.05137 (2017).}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Semi-supervised learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Semi-supervised learning}

  \framesubtitle{Concept}

  \begin{center}
    \includegraphics[width=4cm]{images/semi_supervised_learning.png}
  \end{center}

  \bigskip

  The idea of semi-supervised learning is to use
  \textcolor{blue}{\emph{unlabelled data}} to improve our model.

  \bigskip

  {\scriptsize \textit{Image from https://en.wikipedia.org/wiki/Semi-supervised\_learning}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Semi-supervised learning}

  \framesubtitle{Concept}

  The main concept of semi-supervised learning is to train a
  \textcolor{blue}{weaker student} to imitate a
  \textcolor{blue}{stronger teacher}.

  \bigskip

  Technically, we apply \emph{mean-squared error} or a
  \emph{Kullback-Liebler divergence} between the logits output by the
  student and the teacher. We typically alternate between supervised
  and semi-supervised steps of training.

  \bigskip

  The goal of this method is to \textcolor{blue}{propagate labels} and
  improve \textcolor{blue}{noise invariance}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Semi-supervised learning}

  \framesubtitle{Computer vision}

  \begin{center}
    \input{figures/semi_supervised_learning.tex}
  \end{center}

  \bigskip

  We do not need a label for the clean image, we want to teach the
  model to be \textcolor{blue}{noise invariant}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Conclusion}

  \begin{enumerate}
    \item Establish a baseline using basic algorithms (naive Bayes,
      logistic regression, random forest, etc.).
      \medskip
    \item Choose a model architecture (MLP, CNN, RNN, Transformer).
      \medskip
    \item Try to find (or build) a pre-trained version of this model
      that performs a related task and retrain it to your
      problem (\emph{transfer learning}).
      \medskip
    \item Try to find a related auxiliary task to regularize to
      improve the model learning abilities. (\emph{multi-task
        learning}).
      \medskip
    \item Once the performance of the model is relatively good, try to
      use unlabelled data to improve performances
      (\emph{semi-supervised learning}).
  \end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{References}

  \fontsize{6pt}{7.2}\selectfont

  \begin{itemize}
  \item \href{https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0}{https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0}
  \item Radford, Alec, et al. "Improving language understanding by generative pre-training." URL
    \href{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}{Article pdf link}
    \href{https://blog.openai.com/language-unsupervised/}{Blog post} (2018).
  \item Devlin, Jacob, et al. "Bert: Pre-training of deep
    bidirectional transformers for language understanding." arXiv
    preprint arXiv:1810.04805 (2018).
  \item Poplin, Ryan, et al. "Predicting cardiovascular risk factors
    from retinal fundus photographs using deep learning."  arXiv
    preprint arXiv:1708.09843 (2017).
  \item Kaiser, Lukasz, et al. "One model to learn them all."  arXiv
    preprint arXiv:1706.05137 (2017).
  \item Tarvainen, Antti, et al. "Mean teachers are better
    role models: Weight-averaged consistency targets improve
    semi-supervised deep learning results." Advances in neural
    information processing systems (2017).
  \item Miyato, Takeru, et al. "Virtual adversarial training: a
    regularization method for supervised and semi-supervised
    learning." IEEE transactions on pattern analysis and machine
    intelligence (2018).
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
