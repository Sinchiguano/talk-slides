\documentclass[9pt]{beamer}

\input{packages.tex}
\input{colors.tex}
\input{commands.tex}

\usetheme{Boadilla}
\title{Generative pre-training of transformer networks}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Disaitek\\Intel Software Innovator\\\vspace{0.3cm}@rodgzilla\\github.com/rodgzilla}
\date{01/23/2018}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
%% \setbeamertemplate{footline}[frame number]{}

%% In this talk, I will present two recent research articles from openAI
%% and Google AI Language about transfer learning in NLP and their
%% implementation.

%% Historically, transfer learning for NLP neural networks has been
%% limited to reusing pre-computed word embeddings. Recently, a new trend
%% appeared, much closer to what transfer learning looks like in computer
%% vision, consisting in reusing a much larger part of a pre-trained
%% network. This approach allows to reach state of the art results on
%% many NLP tasks with minimal code modification and training time. In
%% this presentation, I will present the underlying architectures of
%% these models, the generic pre-training tasks and an example of using
%% such network to complete a NLP task.
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Traditional architectures for NLP}

  \begin{figure}
    \begin{tikzpicture}[xscale = 5, yscale = 2.7]
      \node (cnnText) at (0, 0) {
        CNN
      };

      \node (cnnPic) at (1, 0) {
        \includegraphics[width = 5cm]{images/cnn.png}
      };

      \node (dilatedCnnText) at (0, -1) {
        Dilated CNN
      };

      \node (dilatedCnnPic) at (1, -1) {
        \includegraphics[width = 4.5cm]{images/dilated_cnn.png}
      };

      \node (rnnText) at (0, -2) {
        RNN
      };

      \node (rnnPic) at (1, -2) {
        \includegraphics[width = 5cm]{images/rnn.png}
      };
    \end{tikzpicture}
  \end{figure}

  {\tiny Image from https://techblog.gumgum.com/articles/deep-learning-for-natural-language-processing-part-2-rnns and http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Attention mechanisms}

  \framesubtitle{Concept}

  \begin{figure}
    \begin{tikzpicture}[xscale = 6]
      \node (f1) at (0, 0) {
        \includegraphics[angle = -90, width = 4.5cm]{images/attention_1.png}
      };

      \node (f2) at (1, 0) {
        \includegraphics[angle = -90, width = 4.5cm]{images/attention_2.png}
      };
    \end{tikzpicture}

  \end{figure}

  {\tiny Image from https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Attention mechanisms}

  \framesubtitle{Scaled Dot-Product Attention}

  \medskip

  To compute the next word in the translation, the attention mechanism
  creates a vector using the source sentence and what has been
  generated so far.

  \begin{figure}
    \scalebox{0.85}{
      \input{figures/attention_translation.tex}
    }
  \end{figure}

  $Q$, $K$ and $V$ are respectively the query, key and value vectors.

  \bigskip

  \[
  \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}}) V.
  \]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Attention mechanisms}

  \framesubtitle{Multi-Head Attention}

  \begin{figure}
    \includegraphics[height = 4cm]{images/multi_head_attention.png}
  \end{figure}

  \begin{align*}
    \text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_{1}, \dots, \text{head}_{h}) \\
    \text{where} \quad \text{head}_{i} &= \text{Attention}(QW^{Q}_{i}, KW^{K}_{i},
  VW^{V}_{i})
  \end{align*}

  \bigskip

  where the projections $W^{Q}_{i}$, $W^{K}_{i}$ and $W^{V}_{i}$ are
  parameter matrices.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transformer network}

  \framesubtitle{Original transformer}

  \begin{figure}
    \includegraphics[width = 5cm]{images/base_transformer.png}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Transformer network}

  \framesubtitle{OpenAI multi-layer decoder}

  \begin{figure}
    \input{figures/decoder_figure.tex}
  \end{figure}

  The \emph{Text Prediction} and \emph{Task classifier} heads take
  $h_{n}$ as input.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Unsupervised pre-training task}

  \framesubtitle{Language modeling}

  \begin{figure}
    \scalebox{0.8}{
      \input{figures/language_modeling.tex}
    }
  \end{figure}

  \begin{align*}
    P(u) &= \text{softmax}(h_{n}W_{e}^{T})\\
    L_{1}(\mathcal{U}) &= \sum_{i} \text{log} \,P(u_{i} | u_{i - k}, \dots, u_{i - 1}; \Theta)
  \end{align*}

  \begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries Hardware}]
  \item[Dataset] BooksCorpus (7000 books, $\sim$ 800M words, $\sim$ 5GB of text),
  \item[Duration] 1 month,
  \item[Hardware] 8 GPUs.
  \end{description}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Supervised fine-tuning}

  \framesubtitle{Multitask learning}

  \fontsize{7.5pt}{7.2}\selectfont

  \begin{figure}
    \input{figures/finetuning.tex}
  \end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Results on standard datasets}

  New state of the art on the following tasks:

  \bigskip

  \begin{itemize}
  \item Textual Entailment
    \begin{itemize}
      \footnotesize
    \item SNLI $89.3 \rightarrow 89.9$
    \item MNLI Matched $80.6 \rightarrow 82.1$
    \item MNLI Mismatched $80.1 \rightarrow 81.4$
    \item SciTail $83.3 \rightarrow 88.3$
    \item QNLI $82.3 \rightarrow 88.1 $
    \end{itemize}
    \smallskip
  \item Semantic Similarity
    \begin{itemize}
      \footnotesize
    \item STS-B $81.0 \rightarrow 82.0$
    \item QQP $66.1 \rightarrow 70.3$
    \end{itemize}
    \smallskip
  \item Reading Comprehension
    \begin{itemize}
      \footnotesize
    \item RACE $53.3 \rightarrow 59.0$
    \end{itemize}
    \smallskip
  \item Commonsense Reasoning
    \begin{itemize}
      \footnotesize
    \item ROCStories $77.6 \rightarrow 86.5$
    \item COPA $71.2 \rightarrow 78.6$
    \end{itemize}
    \smallskip
  \item Linguistic Acceptability
    \begin{itemize}
      \footnotesize
      \item CoLA $35.0 \rightarrow 45.4$
    \end{itemize}
    \smallskip
  \item Multi-Task Benchmark
    \begin{itemize}
      \footnotesize
      \item GLUE $68.9 \rightarrow 72.8$
    \end{itemize}
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Input formatting}

  \begin{figure}
    \includegraphics[width = 8cm]{images/input_formatting.png}
  \end{figure}

  Two possible input shape:

  \begin{itemize}
  \item \texttt{(batch\_idx, token\_idx, 2)}
  \item \texttt{(batch\_idx, sequence\_idx, token\_idx, 2)}
  \end{itemize}

  \bigskip

  The \texttt{2} is there to select either the token embedding or its
  corresponding position embedding.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Input formatting}

  \begin{lstlisting}
def transform_imdb(X, encoder, max_len, n_vocab, n_special,
                   n_ctx):
    n_batch   = len(X)
    xmb       = np.zeros((n_batch, n_ctx, 2), dtype = np.int32)
    mmb       = np.zeros((n_batch, n_ctx), dtype = np.float32)
    start     = encoder['_start_']
    clf_token = encoder['_classify_']
    for i, x in enumerate(X):
        x_with_tokens   = [start] + x[:max_len] + [clf_token]
        l_x             = len(x_with_tokens)
        xmb[i, :l_x, 0] = x_with_tokens
        mmb[i, :l_x]    = 1
    pos_emb_start = n_vocab + n_special
    xmb[:, :, 1]  = np.arange(
        pos_emb_start,
        pos_emb_start + n_ctx
    )

    return xmb, mmb
  \end{lstlisting}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding}

  \begin{figure}
    \includegraphics[width = 8cm]{images/bert.png}
  \end{figure}

  \emph{BERT} is an improvement on the GPT. The main differences are:
  \begin{itemize}
  \item Bidirectional training,
  \item Different pre-training tasks (masked language model and next
    sentence prediction),
  \item Trained on a much bigger corpus (BookCorpus (800M words) \textbf{+
    Wikipedia (2500M words)}),
  \item 3 $\times$ as many parameters for the large version,
  \item Pre-trained model for 102 languages.
  \end{itemize}

  Google's BERT produces 11 new SOTAs on top of the 9 of OpenAI's GPT.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{References}

  \fontsize{6pt}{7.2}\selectfont

  \begin{itemize}
  \item Tensorflow GPT: \href{https://github.com/openai/finetune-transformer-lm}{https://github.com/openai/finetune-transformer-lm}

  \item Tensorflow BERT: \href{https://github.com/google-research/bert}{https://github.com/google-research/bert}

  \item PyTorch GPT: \href{https://github.com/huggingface/pytorch-openai-transformer-lm}{https://github.com/huggingface/pytorch-openai-transformer-lm}

  \item PyTorch BERT: \href{https://github.com/huggingface/pytorch-pretrained-BERT}{https://github.com/huggingface/pytorch-pretrained-BERT}

  \item IMDB movie review classification: \href{https://github.com/rodgzilla/pytorch-openai-transformer-lm/tree/movie_reviews_classification}{https://github.com/rodgzilla/pytorch-openai-transformer-lm/tree/movie\_reviews\_classification}

  \item Bai, Shaojie, J. Zico Kolter, and Vladlen Koltun. "An
    empirical evaluation of generic convolutional and recurrent
    networks for sequence modeling." arXiv preprint arXiv:1803.01271
    (2018).

  \item Vaswani, Ashish, et al. "Attention is all you need." Advances
    in Neural Information Processing Systems. 2017.

  \item Radford, Alec, et al. "Improving language understanding by generative pre-training." URL
    \href{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}{Article pdf link}
    \href{https://blog.openai.com/language-unsupervised/}{Blog post} (2018).

  \item Devlin, Jacob, et al. "BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding." arXiv
    preprint arXiv:1810.04805 (2018).
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
