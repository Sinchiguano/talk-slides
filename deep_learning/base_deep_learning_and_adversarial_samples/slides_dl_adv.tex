\documentclass[9pt]{beamer}

\input{packages.tex}
\input{colors.tex}
\input{commands.tex}

\usetheme{Boadilla}
\title{Adversarial examples in deep learning}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Disaitek\\Intel
  Software Innovator\\\vspace{0.3cm}@rodgzilla\\github.com/rodgzilla}
\date{28/02/2018}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
%% \setbeamertemplate{footline}[frame number]{}


\begin{document}

%% Deep learning is used in more critical systems every day, such as
%% self-driving cars and video surveillance. The purpose of this talk
%% is to show how deep neural networks can be fooled using
%% specifically crafted inputs and how to try to defend these kinds of
%% attack.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Machine learning}

  \framesubtitle{Supervised learning}

  Machine learning is a subfield of artificial intelligence.

  \bigskip

  \begin{description}
    \item[Intuitively] We want to \emph{learn from} and \emph{make predictions
    on} data.

    \item[Technically] We want to build a model that approximate well
      (\textit{e.g.} minimize a loss function) an unknown function.
  \end{description}

  \bigskip

  It is important to note that the function we want to approximate may
  or may not have a closed form.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Application examples}

  \framesubtitle{Supervised learning}

  \begin{itemize}
    \item Regression

      \begin{center}
        \begin{tabular}{cc}
          \textcolor{blue}{Polynomial} & $(x, y, z) \to f(x, y, z)$ \\[0.5cm]
          \textcolor{blue}{House price} & (surface, nb rooms, city) $\to$ price \\[0.5cm]
        \end{tabular}
      \end{center}

    \item Classification

      \begin{center}
        \begin{tabular}{cc}
          \textcolor{blue}{Image classification} & pixel values $\to$ cat or dog \\[0.5cm]
          \textcolor{blue}{Text classification} & list of words $\to$ spam or valid email
        \end{tabular}
      \end{center}
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Deep learning}

  \textcolor{blue}{Deep learning} is a subfield of machine learning in
  which we use \textcolor{blue}{artificial neural networks} to make
  predictions.

  \bigskip

  An artificial neural networks is a computation model loosely based
  on the human brain. It aims to mimic electric signals travelling
  through neurons in order to make computations.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Artificial neural network}

  \framesubtitle{Neuron}

  \begin{center}
    \scalebox{0.9}{
      \input{figures/neuron.tex}
    }
  \end{center}

  \[
  y = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + w_{5}x_{5} + w_{6}x_{6} + b
  \]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Artificial neural network}

  \framesubtitle{Network}

  \begin{center}
    \scalebox{0.7}{
      \input{figures/network.tex}
    }
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Problem with this definition}

  We now have a quite complicated framework to compute
  \textcolor{blue}{linear functions}.

  \bigskip

  \begin{center}
    \begin{tabular}{cc}
      \textcolor{blue}{Neuron} & linear function \\
      \textcolor{blue}{Neural network} & linear combination of neuron outputs \\
    \end{tabular}
  \end{center}

  \bigskip

  To approximate non linear functions, we would like to have a non
  linear model.

  \bigskip

  \uncover<2->{\textcolor{blue}{Solution}: add nonlinearity to neurons.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neuron with activation}

  \begin{center}
    \scalebox{0.6}{
      \input{figures/nonlinear_neuron.tex}
    }
  \end{center}

  \uncover<2->{
    \[
    A(x) = \begin{cases}
        0 & \text{if } x < 0 \\
        1 & \text{otherwise}
    \end{cases}
    \]
  }

  \uncover<3->{
    \[
    y = A(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + w_{5}x_{5} + w_{6}x_{6} + b)
    \]
  }
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Computation example}

  \framesubtitle{Binary AND gate}

  \begin{center}
    \scalebox{1}{
      \input{figures/and_neuron.tex}
    }
  \end{center}

  We want to set $w_{1}, w_{2}$ and $b$ such that:

  \[
  A(w_{1}x_{1} + w_{2}x_{2} + b) = x_{1} \text{ AND } x_{2}
  \]

  \uncover<3->{
    \[
    x_{0} = 0, x_{1} = 1. \quad y = A(0 + 1 - 1.5) = A(-0.5) = 0
    \]
  }

  \uncover<4->{
    \[
    x_{0} = 1, x_{1} = 1. \quad y = A(1 + 1 - 1.5) = A(0.5) = 1
    \]
  }
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Computation example}

  \framesubtitle{Binary XOR gate}

  \begin{center}
    \scalebox{0.7}{
      \input{figures/xor_neuron.tex}
    }
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model complexity}

  One way to measure the complexity of a neural network is its number
  of parameters.

  \bigskip

  \begin{itemize}
    \uncover<2->{
    \item XOR network: 9 parameters
    }
    \bigskip
    \uncover<3->{
    \item dogs vs cats pictures (VGG16 network): 138,357,544 parameters
    }
  \end{itemize}

  \bigskip
  \uncover<4->{We should probably look for a way to tune these
    parameters automatically otherwise it is going to get
    \emph{really} boring \emph{really} quickly.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Parameter tuning}

  \framesubtitle{Line approximation}

  We have a few sample points and we want to find a line that
  approximate these points.

  \begin{center}
    \scalebox{0.8}{
      \input{figures/scatter_01.tex}
    }
  \end{center}

  Our model is just a line

  \[
  y_{pred} = \textcolor{red}{a}x + \textcolor{red}{b}
  \]

  \bigskip

  We want to find $a$ and $b$ to best match our samples.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Parameter tuning}

  \framesubtitle{Gradient descent concept}

  \begin{center}
    \scalebox{0.8}{
      \input{figures/parabola_1.tex}
    }
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Parameter tuning}

  \framesubtitle{Loss definition}

  First we have a define a \textcolor{blue}{loss} that measures how
  good our predictions are.

  \[
  l(x, y, a, b) = (y - y_{pred})^{2} = (y - (a x + b))^{2}
  \]

  \bigskip

  and now, we compute how the loss is affected by small changes of $a$ and
  $b$:

  \medskip

  \[
  \frac{dl}{da} = 2 x (ax + b - y) \qquad \qquad \frac{dl}{db} = 2 (ax + b - y)
  \]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Parameter tuning}

  \framesubtitle{Gradient descent application}

  We start with random values: $a = -2$ and $b = 1$

  \begin{center}
    \scalebox{0.7}{
      \input{figures/scatter_02.tex}
    }
  \end{center}

  \[
  l(x, y, a, b) = (y - y_{pred})^{2} = (y - (a x + b))^{2}
  \]


  Then, we compute the average of the gradient along all $(x, y)$ couples

  \begin{overprint}
    \onslide<1> \[ a = -2.00 \quad b = 1.00 \quad \overline{\frac{dl}{da}} = -1.07 \quad \overline{\frac{dl}{db}} = -1.37 \quad \overline{l(x, y, a, b)} = 0.860367 \]
    \onslide<2> \[ a = -1.18 \quad b = 1.30 \quad \overline{\frac{dl}{da}} = -0.17 \quad \overline{\frac{dl}{db}} = 0.09 \quad \overline{l(x, y, a, b)} = 0.159420 \]
    \onslide<3> \[ a = -0.82 \quad b = 1.10 \quad \overline{\frac{dl}{da}} = -0.13 \quad \overline{\frac{dl}{db}} = 0.07 \quad \overline{l(x, y, a, b)} = 0.088204 \]
    \onslide<4> \[ a = -0.54 \quad b = 0.94 \quad \overline{\frac{dl}{da}} = -0.09 \quad \overline{\frac{dl}{db}} = 0.05 \quad \overline{l(x, y, a, b)} = 0.048802 \]
    \onslide<5> \[ a = -0.34 \quad b = 0.83 \quad \overline{\frac{dl}{da}} = -0.07 \quad \overline{\frac{dl}{db}} = 0.04 \quad \overline{l(x, y, a, b)} = 0.027001 \]
    \onslide<6> \[ a = -0.19 \quad b = 0.75 \quad \overline{\frac{dl}{da}} = -0.05 \quad \overline{\frac{dl}{db}} = 0.03 \quad \overline{l(x, y, a, b)} = 0.014939 \]
    \onslide<7> \[ a = -0.08 \quad b = 0.68 \quad \overline{\frac{dl}{da}} = -0.04 \quad \overline{\frac{dl}{db}} = 0.02 \quad \overline{l(x, y, a, b)} = 0.008266 \]
    \onslide<8> \[ a = 0.01 \quad b = 0.64 \quad \overline{\frac{dl}{da}} = -0.03 \quad \overline{\frac{dl}{db}} = 0.02 \quad \overline{l(x, y, a, b)} = 0.004573 \]
    \onslide<9> \[ a = 0.07 \quad b = 0.60 \quad \overline{\frac{dl}{da}} = -0.02 \quad \overline{\frac{dl}{db}} = 0.01 \quad \overline{l(x, y, a, b)} = 0.002530 \]
    \onslide<10> \[ a = 0.12 \quad b = 0.58 \quad \overline{\frac{dl}{da}} = -0.02 \quad \overline{\frac{dl}{db}} = 0.01 \quad \overline{l(x, y, a, b)} = 0.001400 \]
    \onslide<11> \[ a = 0.15 \quad b = 0.56 \quad \overline{\frac{dl}{da}} = -0.01 \quad \overline{\frac{dl}{db}} = 0.01 \quad \overline{l(x, y, a, b)} = 0.000775 \]
    \onslide<12> \[ a = 0.18 \quad b = 0.54 \quad \overline{\frac{dl}{da}} = -0.01 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000429 \]
    \onslide<13> \[ a = 0.19 \quad b = 0.53 \quad \overline{\frac{dl}{da}} = -0.01 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000237 \]
    \onslide<14> \[ a = 0.21 \quad b = 0.52 \quad \overline{\frac{dl}{da}} = 0.00 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000131 \]
    \onslide<15> \[ a = 0.22 \quad b = 0.52 \quad \overline{\frac{dl}{da}} = 0.00 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000073 \]
    \onslide<16> \[ a = 0.23 \quad b = 0.51 \quad \overline{\frac{dl}{da}} = 0.00 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000040 \]
    \onslide<17> \[ a = 0.23 \quad b = 0.51 \quad \overline{\frac{dl}{da}} = 0.00 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000022 \]
    \onslide<18> \[ a = 0.24 \quad b = 0.51 \quad \overline{\frac{dl}{da}} = 0.00 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000012 \]
    \onslide<19> \[ a = 0.24 \quad b = 0.51 \quad \overline{\frac{dl}{da}} = 0.00 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000007 \]
    \onslide<20> \[ a = 0.24 \quad b = 0.50 \quad \overline{\frac{dl}{da}} = 0.00 \quad \overline{\frac{dl}{db}} = 0.00 \quad \overline{l(x, y, a, b)} = 0.000004 \]
  \end{overprint}

  \bigskip

  And update $a$ and $b$ by substracting a small proportion of their
  gradient.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Parameter tuning}

  \framesubtitle{Problem with ANN}

  Using gradient descent, we know how to minimize (or at least reach a
  local minima) a differentiable function.

  \pause
  \bigskip

  \textcolor{red}{Problem}: The function computed by our neural network is not
  differentiable because of the activation function.

  \[
  A(x) = \begin{cases}
    0 & \text{if } x < 0 \\
    1 & \text{otherwise}
  \end{cases}
  \]

  \medskip

  \pause

  \textcolor{blue}{Solution}: We replace it by a differentiable
  function that does the same job.

  \begin{center}
    \begin{tikzpicture}[xscale = 4]
      \node (formula) (0, 0){
        \scalebox{1.3}{
          $A(x) = \frac{1}{1 + e^{-x}}$
        }
      } ;

      \node (graph) at (1, 0){
        \scalebox{0.6}{
          \input{figures/sigmoid.tex}
        }
      };
    \end{tikzpicture}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is an adversarial example?}

  An \emph{adversarial example} is a sample of input data which has
  been modified \emph{very slightly} in a way that is intended to
  cause a machine learning classifier to misclassify it.

  \bigskip

  \pause

  \begin{center}
    \includegraphics[trim={2pt 2pt 2pt 0}, clip, width =
      \linewidth]{images/adversarial_example_wig.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Basic concept}

  \begin{center}
    \scalebox{0.8}{
      \input{figures/parabola_2.tex}
    }
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Gradient descent}

%%   \framesubtitle{Model optimization}

%%   \begin{center}
%%     \scalebox{0.5}{
%%       \input{figures/scatter_01.tex}
%%     }
%%   \end{center}

%%   We have a set of points that we want to approximate with a line.

%%   \[
%%   y = ax + b
%%   \]

%%   \pause

%%   First we choose a loss that measures how good our predictions are.

%%   \[
%%   L(x, y, a, b) = (y - (a x + b))^{2}
%%   \]

%%   \pause

%%   We compute how the loss is affected by small changes of $a$ and $b$.

%%   \[
%%   \frac{\d L}{\d a} = 2 x (ax + b - y) \qquad \qquad
%%   \frac{\d L}{\d b} = 2 (ax + b - y)
%%   \]

%%   And we update $a$ and $b$ iteratively until we reach a satisfying
%%   result (\textit{i.e.} average loss low enough for our data points).

%% %  a = -0.08 \quad b = 0.68
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Being evil}

  \vspace{-0.5cm}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/scatter_02.tex}
    }
  \end{center}

  In our previous example, we have modified \textcolor{red}{the model}
  in order to minimize the loss.

  \[
  y = \textcolor{red}{a}x + \textcolor{red}{b}
  \]

  \pause

  Now suppose we are an attacker who wants to maximize the loss of a
  model, its \textcolor{red}{parameters} being fixed. The only thing
  we can modify are the \textcolor{blue}{inputs}.

  \[
  L(\textcolor{blue}{x}, y, a, b) = (y - (a \textcolor{blue}{x} + b))^{2}
  \]

  \pause

  In order to do this, we compute how the loss is affected by small
  changes of the input.

  \[
  \frac{\d L}{\d x} = 2 a (ax + b - y)
  \]

  We can now make \emph{imperceptible} changes to an input that will
  increase the loss value.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural networks}

  Everything works the same way when working with a neural network on
  an image classification task.

  \bigskip

  We also have a differentiable \emph{loss function} (often
  \emph{categorical cross entropy}), model \emph{parameters} (the
  \emph{weights} of the neural network) and \emph{inputs} (\emph{pixel
    values} in the case of images) that we can modify to increase the
  loss.

  \bigskip

  The ultimate goal is to make our input cross the \emph{decision
    boundary}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Attack}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Attacks}

%%   \begin{description}
%%   \item[Random noise] Does not work
%%   \item[FGSM] Good but can be well defended by training the network
%%     with adversarial samples
%%   \item[Iterative FGSM] Higher error than FGSM for an equivalent
%%     $\varepsilon$ but less transferability. I-FGSM produces weaker
%%     black-box attacks.
%%   \item[Targeted FGSM] Aims at fooling a model into outputting a given
%%     target class.
%%   \item[RAND + FGSM] Significant improvements against adversarially
%%     trained models. RAND+FGSM transfers at lower rates than FGSM
%%     examples. Unsing RAND+FGSM to adverarially train networks does not
%%     improve their defense against RAND+FGSM.
%%   \end{description}
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \framesubtitle{Do we really need to do that?}

  \begin{center}
    \includegraphics[width = 0.7 \linewidth]{images/wait-a-minute.jpg}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \framesubtitle{Yep}

  \begin{center}
    \includegraphics[width = \linewidth]{images/random_perturbation.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method [Goodfellow et al. 2015]}

  Let $x$ be the original image, $\theta$ the parameters of the model,
  $y$ the target associated with $x$ and $L(\theta, x, y)$ the loss
  function.

  \bigskip

  We compute the gradient of the loss function according to the input
  pixels.

  \[
  \nabla_{x} L(\theta, x, y)
  \]

  \bigskip

  The perturbation is the signs of these derivatives multiplied by a
  small number $\varepsilon$.

  \[
  \eta = \varepsilon \text{ sign}(\nabla_{x} L(\theta, x, y))
  \]

  \bigskip

  The final adversarial sample is the sum of the original image and
  the perturbation.

  \[
  x_{adv} = x + \eta
  \]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method}

  \framesubtitle{\textbf{VGG16} network}

  \begin{center}
    \input{figures/FGSM.tex}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box attack [Papernot et al. 2016]}

  \framesubtitle{or good luck getting gradients out of your
    self-driving car}

  \begin{center}
    \includegraphics[width = 5cm]{images/cool_story_bro_1.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box attack [Papernot et al. 2016]}

  \framesubtitle{Transferability of adversarial samples}

  %% If our interface with the target model is its final output, we
  %% cannot use gradients to create adversarial samples.

  We can train a new model $M'$ to solve the same classification task
  as the target model $M$.

  \pause

  \bigskip

  Once trained, we can create an adversarial sample $x_{adv}'$ for the
  $M'$ model and experiences have shown that $x_{adv}'$ will also fool
  $M$ very often.

  \pause

  \bigskip

  What if we do not have a training set for the target network?
  Well\dots{} build one using $M$ predictions.

  \pause

  \bigskip

  \textit{``After labeling 6,400 synthetic inputs to train our
    substitute (an order of magnitude smaller than the training set
    used by MetaMind) we find that their DNN misclassifies adversarial
    examples crafted with our substitute at a rate of 84.24\%''}

  \smallskip

  - Papernot et al., about their attack on the MetaMind deep neural
  network.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world [Kurakin et
      al. 2017]}

  \framesubtitle{or good luck attacking a self-driving car with
    your USB flash drive}

  \begin{center}
    \includegraphics[width = 5cm]{images/cool_story_bro_2.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world [Kurakin et
      al. 2017]}

  In real world scenarios, the target network does not take our image
  files as input. It acquires the data by the network's system
  (\textit{e.g.} a camera).

  \medskip

  It also works, for free.

  \begin{center}
    \includegraphics[width = 8cm]{images/physical_adversarial_sample.png}
  \end{center}

%  \bigskip

  \textit{``We used images taken from a cell-phone camera as a input
    to an Inception v3 image classification neural network. We showed
    that in such a set-up, a significant fraction of adversarial
    images crafted using the original network are misclassified even
    when fed to the classifier through the camera.''}

  \smallskip

  - Kurakin et al.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Defense}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Defenses}

%%   \begin{description}
%%   \item[Adversarial sample detection] We try to detect whether an
%%     input sample is adversarial or not before classifying it.
%%   \item[Regularization] Training with an adversarial objective
%%     function is an effective regularizer (from [explaining and
%%       harnessing]).
%%   \item[Gradient masking] The goal of gradient masking is to leave the
%%     decision boundaries untouched but damage the gradient used in
%%     white-box attacks.
%%   \item[Distillation and network saturation] These methods are used to
%%     introduce numerical instabilities in gradient computations.
%%   \end{description}
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{White-box defense}

  \framesubtitle{Adversarial training}

  We can generate adversarial samples and train the network to produce
  the correct classification on these new data points.

  \medskip

  \begin{itemize}
  \item \textcolor{green}{Works against FGSM}
  \item \textcolor{red}{Expensive}
  \item \textcolor{red}{Does not defend subtler white-box
    attacks (\textit{e.g.} I-FGSM or RAND+FGSM)}
  \item \textcolor{red}{Does not defend against \underline{black-box
      attack}}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box defense}

  \framesubtitle{Ensemble adversarial training [Tramèr et al. May 2017]}

  Augment training data with adversarial inputs from a number of fixed
  pre-trained models.

  \medskip

  \begin{itemize}
  \item \textcolor{green}{Best defense so far against black-box
    adversary}
  \item \textcolor{red}{Does not defend subtle white-box attacks}
  \end{itemize}

  \medskip

  Error rate from 15.5\% to 3.9\% between adversarial trained and
  ensemble adversarial trained model on MNIST for a black-box attack.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Defending machine learning}

  \framesubtitle{Wide open problem}

  ``Most defenses against adversarial examples that have been proposed
  so far just do not work very well at all, but the ones that do work
  are not adaptive. This means it is like they are playing a game of
  whack-a-mole: they close some vulnerabilities, but leave others
  open.''

  \bigskip

  - Ian Goodfellow, Nicolas Papernot, February 2017
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{References}

  Research papers:
  {\footnotesize
    \begin{enumerate}
    \item Goodfellow, I. J., Shlens, J., \& Szegedy,
      C. (2014). Explaining and harnessing adversarial
      examples. arXiv preprint arXiv:1412.6572.
    \item Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik,
      Z. B., \& Swami, A. (2016). Practical black-box attacks against
      deep learning systems using adversarial examples. arXiv preprint
      arXiv:1602.02697.
    \item Kurakin, A., Goodfellow, I., \& Bengio,
      S. (2016). Adversarial examples in the physical world. arXiv
      preprint arXiv:1607.02533.
    \item Tramèr, F., Kurakin, A., Papernot, N., Boneh, D., \&
      McDaniel, P. (2017). Ensemble Adversarial Training: Attacks and
      Defenses. arXiv preprint arXiv:1705.07204.
    \item Tramèr, F., Papernot, N., Goodfellow, I., Boneh, D., \&
      McDaniel, P. (2017). The Space of Transferable Adversarial
      Examples. arXiv preprint arXiv:1704.03453.
    \end{enumerate}
  }

  \bigskip

  Implementations:
  \begin{enumerate}
  \item github.com/tensorflow/cleverhans
  \item github.com/rodgzilla/machine\_learning\_adversarial\_examples
  \end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Targeted perturbation}

  \framesubtitle{Papernot algorithm}

  This algorithm iteratively computes the adversarial saliency value
  $S(x, t)[i]$ of pixel $i$ of the image $x$ according to the class
  $t$.

  \medskip

  \[
  S(x, t)[i] =
  \begin{cases}
    0 \text{\quad if \quad} \frac{\d L_{t}}{\d x_{i}}(x) < 0
    \text{\quad or \quad} \sum_{j \neq t} \frac{\d L_{j}}{\d x_{i}}(x)
    > 0 \\[0.3cm]
    \frac{\d L_{t}}{\d x_{i}}(x) \, \lvert \sum_{j \neq
      t} \frac{\d L_{j}}{\d x_{i}}(x) \rvert \text{ otherwise.}
  \end{cases}
  \]

  \medskip

  and use it iteratively to produce $x_{adv}$ classified as $t$ by the
  network.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Defensive distillation}

  \begin{center}
    \includegraphics[width = 0.9\linewidth]{images/distillation.png}
  \end{center}

  \medskip

  Training a network with explicit relative information about classes
  prevents models from fitting too tightly to the data.

  \medskip

  {\scriptsize Papernot, N., McDaniel, P., Wu, X., Jha, S., \& Swami,
    A. (2016). Distillation as a defense to adversarial perturbations
    against deep neural networks. In Security and Privacy (SP), 2016
    IEEE Symposium on (pp. 582-597). IEEE.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
