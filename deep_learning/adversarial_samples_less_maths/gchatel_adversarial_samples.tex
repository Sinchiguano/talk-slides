\documentclass[9pt]{beamer}

\input{packages.tex}
\input{colors.tex}
\input{commands.tex}

\usetheme{Boadilla}
\title{Adversarial examples in deep learning}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Disaitek\\Intel Software Innovator\\\vspace{0.3cm}@rodgzilla\\github.com/rodgzilla}
\date{06/07/2017}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
%% \setbeamertemplate{footline}[frame number]{}


\begin{document}

%% Deep learning is used in more critical systems every day, such as
%% self-driving cars and video surveillance. The purpose of this talk
%% is to show how deep neural networks can be fooled using
%% specifically crafted inputs and how to try to defend these kinds of
%% attack.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is an adversarial example?}

  An \emph{adversarial example} is a sample of input data which has
  been modified \emph{very slightly} in a way that is intended to
  cause a machine learning classifier to misclassify it.

  \bigskip

  \pause

  \begin{center}
    \includegraphics[trim={2pt 2pt 2pt 0}, clip, width =
      \linewidth]{images/adversarial_example_wig.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Basic concept}

  \begin{center}
    \scalebox{0.8}{
      \input{figures/parabola.tex}
    }
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Gradient descent}

%%   \framesubtitle{Model optimization}

%%   \begin{center}
%%     \scalebox{0.5}{
%%       \input{figures/scatter_01.tex}
%%     }
%%   \end{center}

%%   We have a set of points that we want to approximate with a line.

%%   \[
%%   y = ax + b
%%   \]

%%   \pause

%%   First we choose a loss that measures how good our predictions are.

%%   \[
%%   L(x, y, a, b) = (y - (a x + b))^{2}
%%   \]

%%   \pause

%%   We compute how the loss is affected by small changes of $a$ and $b$.

%%   \[
%%   \frac{\d L}{\d a} = 2 x (ax + b - y) \qquad \qquad
%%   \frac{\d L}{\d b} = 2 (ax + b - y)
%%   \]

%%   And we update $a$ and $b$ iteratively until we reach a satisfying
%%   result (\textit{i.e.} average loss low enough for our data points).

%% %  a = -0.08 \quad b = 0.68
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Gradient descent}

%%   \framesubtitle{Being evil}

%%   \vspace{-0.5cm}

%%   \begin{center}
%%     \scalebox{0.5}{
%%       \input{figures/scatter_02.tex}
%%     }
%%   \end{center}

%%   In our previous example, we have modified \textcolor{red}{the model}
%%   in order to minimize the loss.

%%   \[
%%   y = \textcolor{red}{a}x + \textcolor{red}{b}
%%   \]

%%   \pause

%%   Now suppose we are an attacker who wants to maximize the loss of a
%%   model, its \textcolor{red}{parameters} being fixed. The only thing
%%   we can modify are the \textcolor{blue}{inputs}.

%%   \[
%%   L(\textcolor{blue}{x}, y, a, b) = (y - (a \textcolor{blue}{x} + b))^{2}
%%   \]

%%   \pause

%%   In order to do this, we compute how the loss is affected by small
%%   changes of the input.

%%   \[
%%   \frac{\d L}{\d x} = 2 a (ax + b - y)
%%   \]

%%   We can now make \emph{imperceptible} changes to an input that will
%%   increase the loss value.
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Neural networks}

%%   Everything works the same way when working with a neural network on
%%   an image classification task.

%%   \bigskip

%%   We also have a differentiable \emph{loss function} (often
%%   \emph{categorical cross entropy}), model \emph{parameters} (the
%%   \emph{weights} of the neural network) and \emph{inputs} (\emph{pixel
%%     values} in the case of images) that we can modify to increase the
%%   loss.

%%   \bigskip

%%   The ultimate goal is to make our input cross the \emph{decision
%%     boundary}.

%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural networks}

  To train and evaluate neural networks, we use a \emph{loss function}
  $L(\theta, x, y)$ with $\theta$ the parameters of the models, $x$ an input and
  $y$ the real value corresponding to $x$. This function measures
  \emph{how good} the prediction of the model is on a specific
  example.

  \bigskip

  \pause

  To \textcolor{blue}{train} a neural network we compute the
  derivative of $L$ according to $\theta$ and use the result to update $\theta$
  in order to \textcolor{blue}{decrease} the loss value.

  \bigskip

  \pause

  To create an \textcolor{red}{adversarial sample}, we compute the
  derivative of $L$ according to $x$ and use the result to update the
  pixel values of $x$ in order to \textcolor{red}{increase} the loss
  value. It happens that such modifications of $x$ often cause the
  network to misclassify it.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Attack}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Attacks}

%%   \begin{description}
%%   \item[Random noise] Does not work
%%   \item[FGSM] Good but can be well defended by training the network
%%     with adversarial samples
%%   \item[Iterative FGSM] Higher error than FGSM for an equivalent
%%     $\varepsilon$ but less transferability. I-FGSM produces weaker
%%     black-box attacks.
%%   \item[Targeted FGSM] Aims at fooling a model into outputting a given
%%     target class.
%%   \item[RAND + FGSM] Significant improvements against adversarially
%%     trained models. RAND+FGSM transfers at lower rates than FGSM
%%     examples. Unsing RAND+FGSM to adverarially train networks does not
%%     improve their defense against RAND+FGSM.
%%   \end{description}
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \framesubtitle{Do we really need to do that?}

  \begin{center}
    \includegraphics[width = 0.7 \linewidth]{images/wait-a-minute.jpg}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \framesubtitle{Yep}

  \begin{center}
    \includegraphics[width = \linewidth]{images/random_perturbation.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method [2015]}

  Let $x$ be the original image, $\theta$ the parameters of the model,
  $y$ the target associated with $x$ and $L(\theta, x, y)$ the loss
  function.

  \bigskip

  We compute the gradient of the loss function according to the input
  pixels.

  \[
  \nabla_{x} L(\theta, x, y)
  \]

  \bigskip

  The perturbation is the signs of these derivatives multiplied by a
  small number $\varepsilon$.

  \[
  \eta = \varepsilon \text{ sign}(\nabla_{x} L(\theta, x, y))
  \]

  \bigskip

  The final adversarial sample is the sum of the original image and
  the perturbation.

  \[
  x_{adv} = x + \eta
  \]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method}

  \framesubtitle{\textbf{VGG16} network}

  \begin{center}
    \input{figures/FGSM.tex}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box attack [2016]}

  \framesubtitle{or good luck getting gradients out of your
    self-driving car}

  \begin{center}
    \includegraphics[width = 5cm]{images/cool_story_bro_1.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box attack [2016]}

  \framesubtitle{Transferability of adversarial samples}

  %% If our interface with the target model is its final output, we
  %% cannot use gradients to create adversarial samples.

  We can train a new model $M'$ to solve the same classification task
  as the target model $M$.

  \pause

  \bigskip

  Once trained, we can create an adversarial sample $x_{adv}'$ for the
  $M'$ model and experiences have shown that $x_{adv}'$ will also fool
  $M$ very often.

  \pause

  \bigskip

  What if we do not have a training set for the target network?
  Well\dots{} build one using $M$ predictions.

  \pause

  \bigskip

  \textit{``After labeling 6,400 synthetic inputs to train our
    substitute (an order of magnitude smaller than the training set
    used by MetaMind) we find that their DNN misclassifies adversarial
    examples crafted with our substitute at a rate of 84.24\%''}

  \smallskip

  - Papernot et al., about their attack on the MetaMind deep neural
  network.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world [2017]}

  \framesubtitle{or good luck attacking a self-driving car with
    your USB flash drive}

  \begin{center}
    \includegraphics[width = 8cm]{images/cool_story_bro_2.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world [2017]}

  In real world scenarios, the target network does not take our image
  files as input. It acquires the data by the network's system
  (\textit{e.g.} a camera).

  \medskip

  It also works, for free.

  \begin{center}
    \includegraphics[width = 8cm]{images/physical_adversarial_sample.png}
  \end{center}

%  \bigskip

  \textit{``We used images taken from a cell-phone camera as a input
    to an Inception v3 image classification neural network. We showed
    that in such a set-up, a significant fraction of adversarial
    images crafted using the original network are misclassified even
    when fed to the classifier through the camera.''}

  \smallskip

  - Kurakin et al.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Robust Physical-World Attacks on Machine Learning Models [2017]}

  \framesubtitle{or good luck perturbating the background in real life}

  \begin{center}
    \includegraphics[width = 8cm]{images/cool_story_bro_3.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Robust Physical-World Attacks on Machine Learning Models [2017]}

  Perturbations can also be constrained to mimic vandalism or art in
  order to go unreported by casual observers.

  \bigskip

  This concept allows the algorithm to create perturbations with much
  bigger magnitudes since we do not care about being perceived anymore

  \begin{center}
    \begin{tikzpicture}[xscale = 6]
      \node(I1) at (0, 0) {
        \includegraphics[width = 3cm]{images/stop_art_stickers.png}
      };

      \node(I2) at (1, 0) {
        \includegraphics[width = 4.6cm]{images/love_stop_hate.png}
      };
    \end{tikzpicture}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial Examples that Fool both Human and Computer
    Vision [2018]}

  \begin{center}
    \includegraphics[width = 8cm]{images/stupid_computer.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial Examples that Fool both Human and Computer
    Vision [2018]}

  What do you see here?

  \begin{center}
    \begin{tikzpicture}
      \onslide<1>{
        \node (firstPic) at (0, 0) {
          \includegraphics[width = 3cm]{images/adv_fool_human_1.png}
        };
      }


      \onslide<2>{
        \node (secondPic) at (0, 0) {
          \includegraphics[width = 6cm]{images/adv_fool_human_2.png}
        };
      }
    \end{tikzpicture}
  \end{center}

  \onslide<2>

  By building neural network architecture that closely match the human
  visual system, the authors have manage to create adversarial samples
  that fool humans.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Defense}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Defense}

  \framesubtitle{Making a network robust to adversarial perturbation}

  \begin{center}
    \includegraphics[width = 9cm]{images/defense.jpg}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box defense}

  \framesubtitle{Denoising strategies}

  Train a neural network to remove adversarial perturbation before
  using it.

  \medskip

  The winning team of the defense track of NIPS 2017 competition
  trained a denoising U-net to remove adversarial noise.

  \bigskip

  \begin{center}
    \includegraphics[width = 10cm]{images/dunet.png}
  \end{center}
  %% \begin{itemize}
  %% \item \textcolor{green}{Best defense so far against black-box
  %%   adversary}
  %% \item \textcolor{red}{Does not defend subtle white-box attacks}
  %% \end{itemize}

  %% \medskip

  %% Error rate from 15.5\% to 3.9\% between adversarial trained and
  %% ensemble adversarial trained model on MNIST for a black-box attack.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{White-box defense}

  \framesubtitle{Adversarial training}

  We can generate adversarial samples and train the network to produce
  the correct classification on these new data points.

  \medskip

  \begin{itemize}
  \item \textcolor{green}{Works against FGSM}
  \item \textcolor{red}{Expensive}
  \item \textcolor{red}{Does not defend subtler white-box
    attacks (\textit{e.g.} I-FGSM or RAND+FGSM)}
  \item \textcolor{red}{Does not defend against \underline{black-box
      attack}}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Defending machine learning}

  \framesubtitle{Still open problem}

  ``No method of defending against adversarial examples is yet
  completely satisfactory. This remains a rapidly evolving research
  area.''

  \bigskip

  - Alexey Kurakin, Ian Goodfellow, Samy Bengio et al., March 2018
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Use of adversarial examples}

  What adversarial samples can be used for?
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Use of adversarial examples}

  \framesubtitle{Make jokes}

  \begin{center}
    \includegraphics[width = 8cm]{images/car_crash.jpg}
  \end{center}
  \bigskip
  OK, but besides this?
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Use of adversarial examples}

  \framesubtitle{ConvNets and ImageNet Beyond Accuracy: Explanations,
    Bias Detection, Adversarial Examples and Model Criticism [2017]}

  \begin{center}
    \includegraphics[width = 10cm]{images/imagenet_bias.png}
  \end{center}

  \bigskip
  Explore the biases of a neural network by analysing the distance
  of samples to the decision boundary using adversarial samples.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Use of adversarial examples}

  \framesubtitle{Virtual adversarial training: a regularization method
    for supervised and semi-supervised learning. [2017]}

  \vspace{1cm}

  \begin{center}
    \includegraphics[width = 3cm]{images/vat.png}
  \end{center}

  \bigskip

  Use unlabelled data to regularize neural network by training it to
  make the same prediction on a unlabelled image and its adversarial
  counterpart.  \\[1.5cm]

  {\footnotesize Image from https://thecuriousaicompany.com/mean-teacher/}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{References}

  Research papers:
  {\footnotesize
    \begin{itemize}
    \item Goodfellow, I. J., Shlens, J., \& Szegedy,
      C. (2014). Explaining and harnessing adversarial
      examples. arXiv preprint arXiv:1412.6572.
    \item Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik,
      Z. B., \& Swami, A. (2016). Practical black-box attacks against
      deep learning systems using adversarial examples. arXiv preprint
      arXiv:1602.02697.
    \item Kurakin, A., Goodfellow, I., \& Bengio,
      S. (2016). Adversarial examples in the physical world. arXiv
      preprint arXiv:1607.02533.
    \item Tramèr, F., Papernot, N., Goodfellow, I., Boneh, D., \&
      McDaniel, P. (2017). The Space of Transferable Adversarial
      Examples. arXiv preprint arXiv:1704.03453.
    \item Miyato, T., Maeda, S. I., Koyama, M., \& Ishii,
      S. (2017). Virtual adversarial training: a regularization method
      for supervised and semi-supervised learning. arXiv preprint
      arXiv:1704.03976.
    \item Tramèr, F., Kurakin, A., Papernot, N., Boneh, D., \&
      McDaniel, P. (2017). Ensemble Adversarial Training: Attacks and
      Defenses. arXiv preprint arXiv:1705.07204.
    \item Evtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B.,
      Prakash, A., Rahmati A. \& Song, D. (2017). Robust Physical-World
      Attacks on Machine Learning Models. arXiv preprint
      arXiv:1707.08945.
    \item Stock, P., \& Cisse, M. (2017). ConvNets and ImageNet Beyond
      Accuracy: Explanations, Bias Detection, Adversarial Examples and
      Model Criticism. arXiv preprint arXiv:1711.11443.
    \item Liao, F., Liang, M., Dong, Y., Pang, T., Zhu, J., \& Hu,
      X. (2017). Defense against Adversarial Attacks Using High-Level
      Representation Guided Denoiser. arXiv preprint arXiv:1712.02976.
    \item Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N.,
      Kurakin, A., Goodfellow, I., \& Sohl-Dickstein,
      J. (2018). Adversarial Examples that Fool both Human and
      Computer Vision. arXiv preprint arXiv:1802.08195.
    \end{itemize}
  }

  \medskip

  Implementations:
  \begin{itemize}
    \item github.com/tensorflow/cleverhans
    \item github.com/rodgzilla/machine\_learning\_adversarial\_examples
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Targeted perturbation}

  \framesubtitle{Papernot algorithm}

  This algorithm iteratively computes the adversarial saliency value
  $S(x, t)[i]$ of pixel $i$ of the image $x$ according to the class
  $t$.

  \medskip

  \[
  S(x, t)[i] =
  \begin{cases}
    0 \text{\quad if \quad} \frac{\d L_{t}}{\d x_{i}}(x) < 0
    \text{\quad or \quad} \sum_{j \neq t} \frac{\d L_{j}}{\d x_{i}}(x)
    > 0 \\[0.3cm]
    \frac{\d L_{t}}{\d x_{i}}(x) \, \lvert \sum_{j \neq
      t} \frac{\d L_{j}}{\d x_{i}}(x) \rvert \text{ otherwise.}
  \end{cases}
  \]

  \medskip

  and use it iteratively to produce $x_{adv}$ classified as $t$ by the
  network.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial patch}

  \begin{center}
    \includegraphics[width = 0.8\linewidth]{images/adversarial_patch.png}
  \end{center}

  \medskip

  Real world adversarial attack on VGG16 using a sticker.

  \medskip

  {\scriptsize Brown, T. B., Mané, D., Roy, A., Abadi, M., \& Gilmer,
    J. (2017). Adversarial patch. arXiv preprint arXiv:1712.09665.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Defensive distillation}

  \begin{center}
    \includegraphics[width = 0.9\linewidth]{images/distillation.png}
  \end{center}

  \medskip

  Training a network with explicit relative information about classes
  prevents models from fitting too tightly to the data.

  \medskip

  {\scriptsize Papernot, N., McDaniel, P., Wu, X., Jha, S., \& Swami,
    A. (2016). Distillation as a defense to adversarial perturbations
    against deep neural networks. In Security and Privacy (SP), 2016
    IEEE Symposium on (pp. 582-597). IEEE.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
