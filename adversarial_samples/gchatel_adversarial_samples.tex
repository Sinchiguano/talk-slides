\documentclass[9pt]{beamer}

\input{packages.tex}
\input{colors.tex}

\usetheme{Boadilla}
\title{Adversarial examples in deep learning}
\author{G. Ch√¢tel}
\date{06/07/2017}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \tableofcontents

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Basic notions}

  \framesubtitle{Adversarial example}

  An \emph{adversarial example} is a sample of input data which has
  been modified \emph{very slightly} in a way that is intended to
  cause a machine learning classifier to misclassify it.

  \bigskip

  \pause

  \begin{center}
    \includegraphics[trim={2pt 2pt 2pt 0}, clip, width =
      \linewidth]{images/adversarial_example_wig.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Basic concept}

  \begin{center}
    \scalebox{0.8}{
      \input{figures/parabola.tex}
    }
  \end{center}

  The curve needs to be \textit{smooth enough} for the gradient
  descent to work.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Model optimization}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/scatter_01.tex}
    }
  \end{center}

  We have a set of points that we want to approximate with a line.

  \[
  y = ax + b
  \]

  \pause

  First we choose a \textcolor{blue}{loss} that measures how good our
  predictions are.

  \[
  l(x, y, a, b) = (y - (a x + b))^{2}
  \]

  \pause

  We compute how the loss is affected by small changes of $a$ and
  $b$:

  \[
  \frac{\mathrm{d}l}{\mathrm{d}a} = 2 x (ax + b - y) \qquad \qquad \frac{\mathrm{d}l}{\mathrm{d}b} = 2 (ax + b - y)
  \]

  And we update $a$ and $b$ iteratively until we reach a satisfying
  result (the average loss for our data points is low enough).

%  a = -0.08 \quad b = 0.68
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Being evil}

  \vspace{-0.5cm}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/scatter_02.tex}
    }
  \end{center}

  In our previous example, we have modified \textcolor{red}{the model}
  in order to minimize the loss.

  \[
  y = \textcolor{red}{a}x + \textcolor{red}{b}
  \]

  \pause

  Now suppose we are an attacker who wants to maximise the loss of a
  model, its \textcolor{red}{parameters} being fixed. The only thing
  we can modify is the \textcolor{blue}{inputs}.

  \[
  l(\textcolor{blue}{x}, y, a, b) = (y - (a \textcolor{blue}{x} + b))^{2}
  \]

  \pause

  In order to do this, we compute how the loss is affected by small
  changes of the input:

  \[
  \frac{\mathrm{d}l}{\mathrm{d}x} = 2 a (ax + b - y)
  \]

  We can now make \emph{imperceptible} changes to an input to make the
  loss grow.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural networks}

  I used a basic regression task to illustrate the concept of
  adversarial samples generation.

  \bigskip

  Everything works the same way when working with a neural network on
  an image classification task.

  \bigskip

  We also have a differentiable \textcolor{red}{loss function} (often
  \textcolor{red}{categorical cross entropy}) and
  \textcolor{blue}{inputs} (\textcolor{blue}{pixel values}) that we
  can modify to increase the loss.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Attack}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Attacks}

  \begin{description}
  \item[Random noise] Does not work
  \item[FGSM] Good but can be well defended by training the network
    with adversarial samples
  \item[Iterative FGSM] Higher error than FGSM for an equivalent
    $\varepsilon$ but less transferability. I-FGSM produces weaker
    black-box attacks.
  \item[Targeted FGSM] Aims at fooling a model into outputting a given
    target class.
  \item[RAND + FGSM] Significant improvements against adversarially
    trained models. RAND+FGSM transfers at lower rates than FGSM
    examples. Unsing RAND+FGSM to adverarially train networks does not
    improve their defense against RAND+FGSM.
  \end{description}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method}

  Move along the derivate away from the correct value as a way to
  maximise the error.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black box attack}

  This is nice but happens if you cannot access the gradients

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world}

  This is nice but in real world scenarios, we are not feeding the
  network with our own data, it is acquired by the network's system
  (using camera for example).

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Defense}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Defenses}

  \begin{description}
  \item[Adversarial sample detection] We try to detect whether an
    input sample is adversarial or not before classifying it.
  \item Training with an adversarial objective function is an
    effective regularizer (from [explaining and harnessing]).
  \item[Gradient masking] The goal of gradient masking is to leave the
    decision boundaries untouched but damage the gradient used in
    white-box attacks.
  \item[Distillation and network saturation] These methods are used to
    introduce numeraical instabilities in gradient computations.
  \end{description}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
