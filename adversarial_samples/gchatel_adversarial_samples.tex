\documentclass[9pt]{beamer}

\input{packages.tex}
\input{colors.tex}
\input{commands.tex}

\usetheme{Boadilla}
\title{Adversarial examples in deep learning}
\author[G. Châtel]{Grégory Châtel\\\vspace{0.3cm}Datategy\\Intel Software Innovator\\\vspace{0.3cm}@rodgzilla\\github.com/rodgzilla}
\date{06/07/2017}

\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
%% \setbeamertemplate{footline}[frame number]{}


\begin{document}

%% Deep learning is used in more critical systems every day, such as
%% self-driving cars and video surveillance. The purpose of this talk
%% is to show how deep neural networks can be fooled using
%% specifically crafted inputs and how to try to defend these kinds of
%% attack.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is an adversarial example?}

  An \emph{adversarial example} is a sample of input data which has
  been modified \emph{very slightly} in a way that is intended to
  cause a machine learning classifier to misclassify it.

  \bigskip

  \pause

  \begin{center}
    \includegraphics[trim={2pt 2pt 2pt 0}, clip, width =
      \linewidth]{images/adversarial_example_wig.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Basic concept}

  \begin{center}
    \scalebox{0.8}{
      \input{figures/parabola.tex}
    }
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Model optimization}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/scatter_01.tex}
    }
  \end{center}

  We have a set of points that we want to approximate with a line.

  \[
  y = ax + b
  \]

  \pause

  First we choose a loss that measures how good our predictions are.

  \[
  L(x, y, a, b) = (y - (a x + b))^{2}
  \]

  \pause

  We compute how the loss is affected by small changes of $a$ and $b$.

  \[
  \frac{\d L}{\d a} = 2 x (ax + b - y) \qquad \qquad
  \frac{\d L}{\d b} = 2 (ax + b - y)
  \]

  And we update $a$ and $b$ iteratively until we reach a satisfying
  result (\textit{i.e.} average loss low enough for our data points).

%  a = -0.08 \quad b = 0.68
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Gradient descent}

  \framesubtitle{Being evil}

  \vspace{-0.5cm}

  \begin{center}
    \scalebox{0.5}{
      \input{figures/scatter_02.tex}
    }
  \end{center}

  In our previous example, we have modified \textcolor{red}{the model}
  in order to minimize the loss.

  \[
  y = \textcolor{red}{a}x + \textcolor{red}{b}
  \]

  \pause

  Now suppose we are an attacker who wants to maximize the loss of a
  model, its \textcolor{red}{parameters} being fixed. The only thing
  we can modify are the \textcolor{blue}{inputs}.

  \[
  L(\textcolor{blue}{x}, y, a, b) = (y - (a \textcolor{blue}{x} + b))^{2}
  \]

  \pause

  In order to do this, we compute how the loss is affected by small
  changes of the input.

  \[
  \frac{\d L}{\d x} = 2 a (ax + b - y)
  \]

  We can now make \emph{imperceptible} changes to an input that will
  increase the loss value.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Neural networks}

  Everything works the same way when working with a neural network on
  an image classification task.

  \bigskip

  We also have a differentiable \textcolor{red}{loss function} (often
  \textcolor{red}{categorical cross entropy}) and
  \textcolor{blue}{inputs} (\textcolor{blue}{pixel values} in the case
  of images) that we can modify to increase the loss.

  \bigskip

  The ultimate goal is to make our input cross the \emph{decision
    boundary}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Attack}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Attacks}

%%   \begin{description}
%%   \item[Random noise] Does not work
%%   \item[FGSM] Good but can be well defended by training the network
%%     with adversarial samples
%%   \item[Iterative FGSM] Higher error than FGSM for an equivalent
%%     $\varepsilon$ but less transferability. I-FGSM produces weaker
%%     black-box attacks.
%%   \item[Targeted FGSM] Aims at fooling a model into outputting a given
%%     target class.
%%   \item[RAND + FGSM] Significant improvements against adversarially
%%     trained models. RAND+FGSM transfers at lower rates than FGSM
%%     examples. Unsing RAND+FGSM to adverarially train networks does not
%%     improve their defense against RAND+FGSM.
%%   \end{description}
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \framesubtitle{Do we really need to do that?}

  \begin{center}
    \includegraphics[width = 0.7 \linewidth]{images/wait-a-minute.jpg}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random noise perturbation}

  \framesubtitle{Yep}

  \begin{center}
    \includegraphics[width = \linewidth]{images/random_perturbation.png}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method [Goodfellow et al. 2015]}

  Let $x$ be the original image, $\theta$ the parameters of the model,
  $y$ the target associated with $x$ and $L(\theta, x, y)$ the loss
  function.

  \bigskip

  We compute the gradient of the loss function according to the input
  pixels.

  \[
  \nabla_{x} L(\theta, x, y)
  \]

  \bigskip

  The perturbation is the signs of these derivatives multiplied by a
  small number $\varepsilon$.

  \[
  \eta = \varepsilon \text{ sign}(\nabla_{x} L(\theta, x, y))
  \]

  \bigskip

  The final adversarial sample is the sum of the original image and
  the perturbation.

  \[
  x_{adv} = x + \eta
  \]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Fast Gradient Sign Method}

  \framesubtitle{\textbf{VGG16} network}

  \begin{center}
    \input{figures/FGSM.tex}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box attack [Papernot et al. 2016]}

  \framesubtitle{or good luck getting gradients out of your
    self-driving car}

  \begin{center}
    \includegraphics[width = 5cm]{images/cool_story_bro_1.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box attack [Papernot et al. 2016]}

  \framesubtitle{Transferability of adversarial samples}

  %% If our interface with the target model is its final output, we
  %% cannot use gradients to create adversarial samples.

  We can train a new model $M'$ to solve the same classification task
  as the target model $M$.

  \pause

  \bigskip

  Once trained, we can create an adversarial sample $x_{adv}'$ for the
  $M'$ model and experiences have shown that $x_{adv}'$ will also fool
  $M$ very often.

  \pause

  \bigskip

  What if we do not have a training set for the target network?
  Well\dots{} build one using $M$ predictions.

  \pause

  \bigskip

  \textit{``After labeling 6,400 synthetic inputs to train our
    substitute (an order of magnitude smaller than the training set
    used by MetaMind) we find that their DNN misclassifies adversarial
    examples crafted with our substitute at a rate of 84.24\%''}

  \smallskip

  - Papernot et al., about their attack on the MetaMind deep neural
  network.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world [Kurakin et
      al. 2017]}

  \framesubtitle{or good luck attacking a self-driving car with
    your USB flash drive}

  \begin{center}
    \includegraphics[width = 5cm]{images/cool_story_bro_2.jpg}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Adversarial examples in the physical world [Kurakin et
      al. 2017]}

  In real world scenarios, the target network does not take our image
  files as input. It acquires the data by the network's system
  (\textit{e.g.} a camera).

  \medskip

  It also works, for free.

  \begin{center}
    \includegraphics[width = 8cm]{images/physical_adversarial_sample.png}
  \end{center}

%  \bigskip

  \textit{``We used images taken from a cell-phone camera as a input
    to an Inception v3 image classification neural network. We showed
    that in such a set-up, a significant fraction of adversarial
    images crafted using the original network are misclassified even
    when fed to the classifier through the camera.''}

  \smallskip

  - Kurakin et al.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Defense}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}
%%   \frametitle{Defenses}

%%   \begin{description}
%%   \item[Adversarial sample detection] We try to detect whether an
%%     input sample is adversarial or not before classifying it.
%%   \item[Regularization] Training with an adversarial objective
%%     function is an effective regularizer (from [explaining and
%%       harnessing]).
%%   \item[Gradient masking] The goal of gradient masking is to leave the
%%     decision boundaries untouched but damage the gradient used in
%%     white-box attacks.
%%   \item[Distillation and network saturation] These methods are used to
%%     introduce numerical instabilities in gradient computations.
%%   \end{description}
%% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{White-box defense}

  \framesubtitle{Adversarial training}

  We can generate adversarial samples and train the network to produce
  the correct classification on these new data points.

  \medskip

  \begin{itemize}
  \item \textcolor{green}{Works against FGSM}
  \item \textcolor{red}{Expensive}
  \item \textcolor{red}{Does not defend subtler white-box
    attacks (\textit{e.g.} I-FGSM or RAND+FGSM)}
  \item \textcolor{red}{Does not defend against \underline{black-box
      attack}}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Black-box defense}

  \framesubtitle{Ensemble adversarial training [Tramèr et al. May 2017]}

  Augment training data with adversarial inputs from a number of fixed
  pre-trained models.

  \medskip

  \begin{itemize}
  \item \textcolor{green}{Best defense so far against black-box
    adversary}
  \item \textcolor{red}{Does not defend subtle white-box attacks}
  \end{itemize}

  \medskip

  Error rate from 15.5\% to 3.9\% between adversarial trained and
  ensemble adversarial trained model on MNIST for a black-box attack.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

  \frametitle{Defending machine learning}

  \framesubtitle{Wide open problem}

  ``Most defenses against adversarial examples that have been proposed
  so far just do not work very well at all, but the ones that do work
  are not adaptive. This means it is like they are playing a game of
  whack-a-mole: they close some vulnerabilities, but leave others
  open.''

  \bigskip

  - Ian Goodfellow, Nicolas Papernot, February 2017
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{References}

  {\footnotesize
    \begin{enumerate}
    \item Goodfellow, I. J., Shlens, J., \& Szegedy,
      C. (2014). Explaining and harnessing adversarial
      examples. arXiv preprint arXiv:1412.6572.
    \item Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik,
      Z. B., \& Swami, A. (2016). Practical black-box attacks against
      deep learning systems using adversarial examples. arXiv preprint
      arXiv:1602.02697.
    \item Kurakin, A., Goodfellow, I., \& Bengio,
      S. (2016). Adversarial examples in the physical world. arXiv
      preprint arXiv:1607.02533.
    \item Tramèr, F., Kurakin, A., Papernot, N., Boneh, D., \&
      McDaniel, P. (2017). Ensemble Adversarial Training: Attacks and
      Defenses. arXiv preprint arXiv:1705.07204.
    \end{enumerate}
  }

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Targeted perturbation}

  \framesubtitle{Papernot algorithm}

  This algorithm iteratively computes the adversarial saliency value
  $S(x, t)[i]$ of pixel $i$ of the image $x$ according to the class
  $t$.

  \medskip

  \[
  S(x, t)[i] =
  \begin{cases}
    0 \text{\quad if \quad} \frac{\d L_{t}}{\d x_{i}}(x) < 0
    \text{\quad or \quad} \sum_{j \neq t} \frac{\d L_{j}}{\d x_{i}}(x)
    > 0 \\[0.3cm]
    \frac{\d L_{t}}{\d x_{i}}(x) \, \lvert \sum_{j \neq
      t} \frac{\d L_{j}}{\d x_{i}}(x) \rvert \text{ otherwise.}
  \end{cases}
  \]

  \medskip

  and use it iteratively to produce $x_{adv}$ classified as $t$ by the
  network.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Defensive distillation}

  \begin{center}
    \includegraphics[width = 0.9\linewidth]{images/distillation.png}
  \end{center}

  \medskip

  Training a network with explicit relative information about classes
  prevents models from fitting too tightly to the data.

  \medskip

  {\scriptsize Papernot, N., McDaniel, P., Wu, X., Jha, S., \& Swami,
    A. (2016). Distillation as a defense to adversarial perturbations
    against deep neural networks. In Security and Privacy (SP), 2016
    IEEE Symposium on (pp. 582-597). IEEE.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
